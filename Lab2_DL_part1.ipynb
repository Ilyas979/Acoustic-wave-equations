{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Lab2_DL_part1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ilyas979/Acoustic-wave-equations/blob/master/Lab2_DL_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyADBY0nDGRx",
        "colab_type": "text"
      },
      "source": [
        "## Lab 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fClubkhpDGRz",
        "colab_type": "text"
      },
      "source": [
        "### Part 1. Overfit it (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gulftrrmDGR0",
        "colab_type": "text"
      },
      "source": [
        "Будем работать с датасетом [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) (*hint: он доступен в torchvision*).\n",
        "\n",
        "Ваша задача состоит в следующем:\n",
        "1. Обучить сеть, которая покажет >= 0.92 test accuracy.\n",
        "2. Пронаблюдать и продемонстрировать процесс переобучения сети с увеличением числа параметров (==нейронов) и/или числа слоев и продемонстрировать это наглядно (например, на графиках).\n",
        "3. Попробовать частично справиться с переобучением с помощью подходящих приемов (Dropout/batchnorm/augmentation etc.)\n",
        "\n",
        "*Примечание*: Пункты 2 и 3 взаимосвязаны, в п.3 Вам прелагается сделать полученную в п.2 сеть менее склонной к переобучению. Пункт 1 является независимым от пунктов 2 и 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STU7_NLiDGR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCfQlyQLDGR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCHSIZE = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "uWD2dH7zDGR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "6d494a53-9597-4d96-86ae-3cbbb71ea059"
      },
      "source": [
        "# transforms\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# datasets\n",
        "trainset = torchvision.datasets.FashionMNIST('./data',\n",
        "    download=True,\n",
        "    train=True,\n",
        "    transform=transform)\n",
        "testset = torchvision.datasets.FashionMNIST('./data',\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=transform)\n",
        "\n",
        "# dataloaders\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCHSIZE,\n",
        "                                        shuffle=True, num_workers=4)\n",
        "\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCHSIZE,\n",
        "                                        shuffle=False, num_workers=4)\n",
        "\n",
        "# constant for classes\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "# helper function to show an image\n",
        "# (used in the `plot_classes_preds` function below)\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    if one_channel:\n",
        "        img = img.mean(dim=0)\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    #plt.figure(figsize=(10, 8))\n",
        "    if one_channel:\n",
        "        plt.imshow(npimg, cmap=\"Greys\")\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "26427392it [00:01, 13848479.27it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 94996.77it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "4423680it [00:01, 3821512.03it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 32563.23it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed6cvkNVDGR8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        #self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv1 = nn.Conv2d(1, 10, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        #self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.conv2 = nn.Conv2d(10, 16, 5)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.bn(self.conv2(x))))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        x = F.log_softmax(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbtOurpSDGR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "#optimizer = optim.Adadelta(net.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibm8mpXIDGSA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "3e9be9f9-3076-463e-a0ff-debcb822ce18"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# default `log_dir` is \"runs\" - we'll be more specific here\n",
        "writer = SummaryWriter('runs/fashion_mnist_experiment_1')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouO4TQV_DGSC",
        "colab_type": "code",
        "outputId": "6e786821-0409-4c99-d53b-a3242722fdb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "source": [
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# create grid of images\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "# show images\n",
        "matplotlib_imshow(img_grid, one_channel=True)\n",
        "\n",
        "# write to tensorboard\n",
        "writer.add_image('four_fashion_mnist_images', img_grid)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAB5CAYAAAAtfwoEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aLX6rk0DGSF",
        "colab_type": "code",
        "outputId": "75b01c09-01b9-4b49-c98b-8e2a1aeff250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "writer.add_graph(net, images)\n",
        "writer.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H94oQa5ADGSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNcsULvGDGSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def select_n_random(data, labels, n=60000):\n",
        "    '''\n",
        "    Selects n random datapoints and their corresponding labels from a dataset\n",
        "    '''\n",
        "    assert len(data) == len(labels)\n",
        "\n",
        "    perm = torch.randperm(len(data))\n",
        "    return data[perm][:n], labels[perm][:n]\n",
        "\n",
        "# select random images and their target indices\n",
        "images, labels = select_n_random(trainset.data, trainset.targets)\n",
        "\n",
        "# get the class labels for each image\n",
        "class_labels = [classes[lab] for lab in labels]\n",
        "\n",
        "# log embeddings\n",
        "features = images.view(-1, 28 * 28)\n",
        "writer.add_embedding(features,\n",
        "                    metadata=class_labels,\n",
        "                    label_img=images.unsqueeze(1))\n",
        "writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgWaoyOmDGSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper functions\n",
        "\n",
        "def images_to_probs(net, images):\n",
        "    '''\n",
        "    Generates predictions and corresponding probabilities from a trained\n",
        "    network and a list of images\n",
        "    '''\n",
        "    output = net(images)\n",
        "    # convert output probabilities to predicted class\n",
        "    _, preds_tensor = torch.max(output, 1)\n",
        "    preds = np.squeeze(preds_tensor.numpy())\n",
        "    return preds, [F.softmax(el, dim=0)[i].item() for i, el in zip(preds, output)]\n",
        "\n",
        "\n",
        "def plot_classes_preds(net, images, labels):\n",
        "    '''\n",
        "    Generates matplotlib Figure using a trained network, along with images\n",
        "    and labels from a batch, that shows the network's top prediction along\n",
        "    with its probability, alongside the actual label, coloring this\n",
        "    information based on whether the prediction was correct or not.\n",
        "    Uses the \"images_to_probs\" function.\n",
        "    '''\n",
        "    preds, probs = images_to_probs(net, images)\n",
        "    # plot the images in the batch, along with predicted and true labels\n",
        "    fig = plt.figure(figsize=(12, 48))\n",
        "    for idx in np.arange(4):\n",
        "        ax = fig.add_subplot(1, 4, idx+1, xticks=[], yticks=[])\n",
        "        matplotlib_imshow(images[idx], one_channel=True)\n",
        "        ax.set_title(\"{0}, {1:.1f}%\\n(label: {2})\".format(\n",
        "            classes[preds[idx]],\n",
        "            probs[idx] * 100.0,\n",
        "            classes[labels[idx]]),\n",
        "                    color=(\"green\" if preds[idx]==labels[idx].item() else \"red\"))\n",
        "    return fig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "3RWSNh51DGSM",
        "colab_type": "code",
        "outputId": "9ff1efd7-cb1e-425b-f74a-8b3eef61a77d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "running_loss = 0.0\n",
        "maxepoch=10\n",
        "acc_count = True\n",
        "for epoch in range(maxepoch):  # loop over the dataset multiple times\n",
        "\n",
        "    for i, data in enumerate(tqdm(trainloader), 0):\n",
        "\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 1000 == 999:    # every 1000 mini-batches...\n",
        "            \n",
        "            # ...log the running loss\n",
        "            # ...log the test loss\n",
        "            with torch.no_grad():\n",
        "                test_loss=0\n",
        "                class_preds_t, class_labels_t = [], []\n",
        "                for data_t in testloader:\n",
        "                    images_t, labels_t = data_t\n",
        "                    output_t = net(images_t)\n",
        "                    test_loss += criterion(output_t, labels_t)\n",
        "                    if acc_count == True:\n",
        "                        _, class_preds_batch_t = torch.max(output_t, 1)\n",
        "                        class_preds_t.append(class_preds_batch_t)\n",
        "                        class_labels_t.append(labels_t)\n",
        "                if acc_count == True:\n",
        "                    test_preds = torch.cat(class_preds_t)\n",
        "                    class_labels_t = torch.cat(class_labels_t)\n",
        "                    _, class_preds_batch = torch.max(outputs, 1)\n",
        "                    writer.add_scalars(f'accuracy', {'training accuracy': (labels==class_preds_batch).numpy().sum() / labels.shape[0],\n",
        "                                        'testing accuracy': (class_labels_t==test_preds).numpy().sum() / class_labels_t.shape[0]},\n",
        "                                        epoch * len(trainloader) + i)\n",
        "            writer.add_scalars(f'loss', {'training loss': running_loss / 1000,\n",
        "                                'testing loss': test_loss / (10000/BATCHSIZE)},\n",
        "                                epoch * len(trainloader) + i)\n",
        "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
        "            # random mini-batch\n",
        "            writer.add_figure('predictions vs. actuals',\n",
        "                            plot_classes_preds(net, inputs, labels),\n",
        "                            global_step=epoch * len(trainloader) + i)\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15000 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "100%|██████████| 15000/15000 [03:24<00:00, 73.19it/s] \n",
            "100%|██████████| 15000/15000 [03:31<00:00, 70.89it/s] \n",
            "100%|██████████| 15000/15000 [03:29<00:00,  7.21it/s] \n",
            "100%|██████████| 15000/15000 [03:37<00:00, 69.04it/s] \n",
            "100%|██████████| 15000/15000 [03:33<00:00, 70.26it/s] \n",
            "100%|██████████| 15000/15000 [03:39<00:00, 68.34it/s] \n",
            "100%|██████████| 15000/15000 [03:59<00:00, 62.70it/s] \n",
            "100%|██████████| 15000/15000 [03:44<00:00, 66.96it/s] \n",
            "100%|██████████| 15000/15000 [03:38<00:00, 68.60it/s] \n",
            "100%|██████████| 15000/15000 [03:40<00:00, 68.09it/s] "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finished Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2HhtzbzDGSO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cc28ce8c-3624-41bb-f46b-af98184c200f"
      },
      "source": [
        "# 1. gets the probability predictions in a test_size x num_classes Tensor\n",
        "# 2. gets the preds in a test_size Tensor\n",
        "# takes ~10 seconds to run\n",
        "class_probs = []\n",
        "class_preds = []\n",
        "class_labels = []\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        output = net(images)\n",
        "        class_probs_batch = [F.softmax(el, dim=0) for el in output]\n",
        "        _, class_preds_batch = torch.max(output, 1)\n",
        "\n",
        "        class_probs.append(class_probs_batch)\n",
        "        class_preds.append(class_preds_batch)\n",
        "        class_labels.append(labels)\n",
        "        \n",
        "test_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
        "test_preds = torch.cat(class_preds)\n",
        "class_labels = torch.cat(class_labels)\n",
        "# helper function\n",
        "def add_pr_curve_tensorboard(class_index, test_probs, test_preds, global_step=0):\n",
        "    '''\n",
        "    Takes in a \"class_index\" from 0 to 9 and plots the corresponding\n",
        "    precision-recall curve\n",
        "    '''\n",
        "    tensorboard_preds = test_preds == class_index\n",
        "    tensorboard_probs = test_probs[:, class_index]\n",
        "\n",
        "    writer.add_pr_curve(classes[class_index],\n",
        "                        tensorboard_preds,\n",
        "                        tensorboard_probs,\n",
        "                        global_step=global_step)\n",
        "    writer.close()\n",
        "\n",
        "# plot all the pr curves\n",
        "for i in range(len(classes)):\n",
        "    add_pr_curve_tensorboard(i, test_probs, test_preds)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lrEeVukDGSQ",
        "colab_type": "code",
        "outputId": "3f7842c6-f531-4f18-b4ef-29d5fc81e7ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(class_labels==test_preds).numpy().sum() / class_labels.shape[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8998"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTefagoZDGST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeOqL-4HDGSV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUuYgqYjDGSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2ic7TzmDGSZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "4255eabc-75ab-4b0c-e950-48e6018f4c61"
      },
      "source": [
        "testloader.dataset"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset FashionMNIST\n",
              "    Number of datapoints: 10000\n",
              "    Root location: ./data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5,), std=(0.5,))\n",
              "           )"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMVsyEwdDGSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26777428-5100-4a76-b847-dc55f211e280"
      },
      "source": [
        "class_preds_batch"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 8, 1, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5pffj7oDGSc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "411be818-06f6-4a63-dadb-bbc724194be5"
      },
      "source": [
        "test_preds"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9, 2, 1,  ..., 8, 1, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-lo0FMADGSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}